{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbf7ced-0d1b-4d25-8b11-ad1d2400409a",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "Langchain is a framework to help us quickly integrate Language Models into applications. Here is a tutorial on some basic components of langchain.\n",
    "\n",
    "Most of the content of the tutorial was taken from [here](https://www.youtube.com/watch?v=P3MAbZ2eMUI). \n",
    "\n",
    "I also included some random mini-projects in this notebook like a small Chain-Of-Thought prompting setup(which didn't yield very interesting results), a program that allows a user to ask questions about a pdf file\n",
    "\n",
    "This notebook uses Ollama as the LLM of choice, but that can easily be swapped out for other models.\n",
    "\n",
    "This notebook will cover:\n",
    "- Embedding Models\n",
    "- Language Models and Prompt Templates\n",
    "- Document Loaders and VectorStores, for Retrieval-Augmented Generation(RAG)\n",
    "- Basic chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8d9d6-cc87-4043-a5a1-da43f8aa3005",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Embedding Models\n",
    "\n",
    "Embeddings represent text as a vector that encodes information about the text. We can generate embeddings, and use embedding models as vector stores. \n",
    "\n",
    "We use Ollama here, must install it before running.\n",
    "\n",
    "References:\n",
    "- [Langchain text embedding with ollama](https://python.langchain.com/docs/integrations/text_embedding/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0713c5ca-6d64-4c5c-9248-648b5805b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320ce568-862a-488d-9d1f-7864db6d25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec42039-fe76-4396-a372-071da1a61bba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embedding texts\n",
    "\n",
    "We can turn texts into vectors by embedding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35be5839-a0ef-4277-8e89-dd807c75a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "# We generate an embedding vector as a list like this\n",
    "emb = embedding_model.embed_query('testing hehehe')\n",
    "print(len(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef99413-6854-4386-abf0-1117cccd55df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2 embeddings\n",
      "Each embedding is 3072 length\n"
     ]
    }
   ],
   "source": [
    "# we can embed multiple documents at once\n",
    "emb = embedding_model.embed_documents(['document 1', 'document 2'])\n",
    "print(f'Got {len(emb)} embeddings')\n",
    "print(f'Each embedding is {len(emb[0])} length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea1022-c69d-49e7-8c50-1bc30566ae90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Vector Stores\n",
    "\n",
    "Vector stores store a collection of documents and their corresponding embeddings, and allow us to compare the embedding of a query text to the embeddings in the vector store. This is used in Retrieval Augmented Generation(RAG).\n",
    "\n",
    "The steps for RAG are:\n",
    "- Embed the query\n",
    "- Find more similar vectors in the vector storage\n",
    "- Take the corresponding documents and append them to the prompt as context\n",
    "- Have the LLM generate a response, given the initial question and added context\n",
    "\n",
    "We will fully implement this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84395c21-76e0-45bc-a05a-25acf058a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector stores. We can look more at the documentation later, but here is a basic way to initialize a vector store\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    ['Pasta is an italian dish', 'C++ is a programming language', 'Kanye West is a rapper'],\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5663f4e8-c47d-44e1-82f8-5c70a09b50ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What food should I eat tonight?\n",
      "Most similar text: Pasta is an italian dish\n",
      "\n",
      "\n",
      "query: What's an example of a guy that raps?\n",
      "Most similar text: Kanye West is a rapper\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever() # here, we can add search_type, search_kwargs\n",
    "\n",
    "# Example\n",
    "# vectorstore.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "#                          search_kwargs={ \"k\": 2, # max docs to return \n",
    "#                                         \"score_threshold\": 0.15, # min threshold for similarity score to return anything\n",
    "#                                        }\n",
    "#                         )\n",
    "\n",
    "# Retrieve the most similar text\n",
    "def print_most_similar_item(text):\n",
    "    print(f'query: {text}')\n",
    "    retrieved_documents = retriever.invoke(text)\n",
    "    print('Most similar text:', retrieved_documents[0].page_content)\n",
    "\n",
    "print_most_similar_item(\"What food should I eat tonight?\")\n",
    "print('\\n')\n",
    "print_most_similar_item(\"What's an example of a guy that raps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232d8d2-9311-4b90-aa67-810fc74dfac4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Language Models and Prompts\n",
    "\n",
    "Language models take text as input, and produce more text. Chat models take chat messages in and return a chat message\n",
    "\n",
    "Prompt templates allow us to interact with the models easier\n",
    "\n",
    "- [video](https://youtu.be/P3MAbZ2eMUI?si=I_VtWWO-H4vrC_sx&t=193)\n",
    "- [FewShotPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03beec3e-6149-4452-b1a6-562e1b56a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "# not really needed\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d49575-9412-4fb3-b9af-2d18a374e67e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Getting outputs from a language model/chat model\n",
    "\n",
    "In the future, most langchain components will be called using an `invoke()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8d2d63-cfea-4086-a488-734aeda69ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model='llama3.2')\n",
    "chat_model = ChatOllama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f821909f-0a87-416b-9742-6ee813a162a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.invoke('say hi')\n",
    "chat_model_result = chat_model.invoke('say hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ecfd11-7638-4068-8090-b87f07b6cbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! How are you today?\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "# this is an AIMessage object\n",
    "print(chat_model_result.content)\n",
    "print(type(chat_model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebb02b6-c8d4-4251-bad1-15869f12cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "print(llm_result) # this is just a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d809c5-f6b1-48d9-85e1-4a482d9fa8d2",
   "metadata": {},
   "source": [
    "## Prompts and prompt templates\n",
    "\n",
    "There are many prompt templates available from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad162257-9eec-400f-be57-1fbdb9406613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using constructor\n",
    "prompt = PromptTemplate(input_variables=['x'], template='Say {x}')\n",
    "\n",
    "# using helper method\n",
    "prompt_2 = PromptTemplate.from_template('say {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87186dcf-d79a-4c42-860c-da93bb3f36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say nong\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(x='nong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761f228b-89db-4fc6-a833-082330471b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nong! (That\\'s \"hello\" in Thai.)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(prompt.format(x='nong')).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1a1be-a24e-459e-9a85-7b68536c05a6",
   "metadata": {},
   "source": [
    "### Constructing a few-shot prompt with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6275af34-8ab8-4471-89ea-dc23c4611fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the fewshotPromptTemplate\n",
    "# https://python.langchain.com/docs/how_to/few_shot_examples/\n",
    "\n",
    "example_1 = {'question': 'Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?',\n",
    "             'answer': 'Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.'}\n",
    "example_2 = {'question': 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?',\n",
    "             'answer': 'The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9'}\n",
    "\n",
    "example_prompt = PromptTemplate.from_template('Question: {question}\\nAnswer: {answer}') # this is how we format examples\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=[example_1, example_2],\n",
    "    example_prompt = example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    prefix=\"{prefix}\",\n",
    "    input_variables=[\"input\", \"prefix\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f274506d-7135-4b20-887f-8d41877041cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are two example questions and answers, followed by a third question. Please answer the third question.\n",
      "\n",
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "Answer: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9\n",
      "\n",
      "Question: Bill has 4 feet on separate legs, each with 5 toes. He loses 1 leg. How many toes does he have left?\n"
     ]
    }
   ],
   "source": [
    "# Few shot prompt just formats all examples, has a suffix and prefix\n",
    "new_question = \"Bill has 4 feet on separate legs, each with 5 toes. He loses 1 leg. How many toes does he have left?\" # 15 toes\n",
    "fs_question = few_shot_prompt.invoke({\"input\": new_question, \n",
    "                                      \"prefix\": \"Below are two example questions and answers, followed by a third question. Please answer the third question.\"})\n",
    "print(fs_question.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f09a5-9eb9-4607-b795-cbe96f2c5a83",
   "metadata": {},
   "source": [
    "### Comparing results of the few-shot prompt vs with no examples\n",
    "\n",
    "Not really much of a difference on this problem. The model is prompted to show it's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97289dc-2bec-4187-a7b3-c43a7cdb50b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this question, we need to find out how many toes Bill had in total before losing a leg.\n",
      "\n",
      "Bill has 4 feet with 5 toes each. So, the total number of toes is:\n",
      "4 x 5 = 20 toes\n",
      "\n",
      "Now, Bill loses one leg, which means he also loses all 5 toes on that leg. To find out how many toes he has left, we subtract 5 from the total number of toes:\n",
      "\n",
      "20 - 5 = 15\n",
      "\n",
      "The answer is 15.\n"
     ]
    }
   ],
   "source": [
    "print(chat_model.invoke(fs_question).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac7b21c3-a526-4d99-beb3-e4221465ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many toes Bill has left, we need to calculate the number of toes he had initially and then subtract the number of toes that were lost.\n",
      "\n",
      "Bill had 4 feet, each with 5 toes. So, the total number of toes he had initially is:\n",
      "\n",
      "4 feet x 5 toes/foot = 20 toes\n",
      "\n",
      "Since Bill lost one leg (and therefore 1 foot), we can now subtract the number of toes that were lost from his initial total:\n",
      "\n",
      "Initial toes: 20\n",
      "Toes lost: 5 (since he lost one leg and each leg has 5 toes)\n",
      "\n",
      "So, the total number of toes Bill has left is:\n",
      "\n",
      "20 - 5 = 15\n",
      "\n",
      "Bill has 15 toes left.\n"
     ]
    }
   ],
   "source": [
    "# somehow, the llama model is able to reason without the few shot examples\n",
    "print(chat_model.invoke(new_question).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b4965-e89c-4c19-9aaa-73477b942508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers prompt the model to provide output in a specified format, and attempt to parse the model-provided output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5db2993-099b-45da-a076-b2a14abfb960",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "class CoTOutput(BaseModel):\n",
    "    model_config = ConfigDict(coerce_numbers_to_str=True)\n",
    "    thought: str=Field(description=\"the chain of thought leading to the solution\")\n",
    "    answer: str=Field(description=\"the solution\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CoTOutput) # a strong output parser, apparently\n",
    "\n",
    "# Let's redo the FewShotPromptTemplate\n",
    "\n",
    "example_1 = {'question': 'Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?',\n",
    "             'thought': 'Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11', \n",
    "             'answer': '11'}\n",
    "example_2 = {'question': 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?',\n",
    "             'thought': 'The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9.', \n",
    "             'answer': '9'}\n",
    "\n",
    "example_prompt = PromptTemplate.from_template('Question: {question}\\nThought: {thought}\\nAnswer: {answer}') # this is how we format examples\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=[example_1, example_2],\n",
    "    example_prompt = example_prompt,\n",
    "    suffix=\"Question: {input}\\n{format_instructions}\",\n",
    "    prefix=\"{prefix}\",\n",
    "    input_variables=[\"input\", \"prefix\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} # partial variables don't have to be passed in every time we call this.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08b17fd1-2f9f-4a4d-9147-19ff2c512db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"Bill could type 54wpm, but broke his arm. Now, he can only type 36wpm. What is the percentage decrease in typing speed that occurred as a result of his broken arm?\"\n",
    "fs_question = few_shot_prompt.invoke({\"input\": new_question, \n",
    "                                      \"prefix\": \"Below are two example questions and answers, followed by a third question. Please answer the third question.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e08c8dce-39f9-403a-8579-4c07206840a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are two example questions and answers, followed by a third question. Please answer the third question.\n",
      "\n",
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Thought: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11\n",
      "Answer: 11\n",
      "\n",
      "Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "Thought: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9.\n",
      "Answer: 9\n",
      "\n",
      "Question: Bill could type 54wpm, but broke his arm. Now, he can only type 36wpm. What is the percentage decrease in typing speed that occurred as a result of his broken arm?\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"thought\": {\"description\": \"the chain of thought leading to the solution\", \"title\": \"Thought\", \"type\": \"string\"}, \"answer\": {\"description\": \"the solution\", \"title\": \"Answer\", \"type\": \"string\"}}, \"required\": [\"thought\", \"answer\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(fs_question.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87b22ff4-f3bd-424b-94fa-af12a890154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Bill could type 54wpm, but broke his arm. Now, he can only type 36wpm. To find the percentage decrease in typing speed, first, calculate the difference in typing speeds: 54 - 36 = 18. Then divide by the original speed and multiply by 100 to get the percentage decrease: (18 / 54) * 100 = 33.333... %. Now round this number to two decimal places to express it as a percentage.\n",
      "Answer: 33.33\n"
     ]
    }
   ],
   "source": [
    "output = chat_model.invoke(fs_question).content # get response as a string\n",
    "parsed_output = parser.parse(output) # this is pretty finicky with CoT prompting\n",
    "print(f\"Thought: {parsed_output.thought}\\nAnswer: {parsed_output.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc2ed2-d651-4c98-aa45-47c8659f33a0",
   "metadata": {},
   "source": [
    "## Putting everything into a simple chain\n",
    "\n",
    "We will talk about chains in a bit, but for now, basically we can pass the output of one component as input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f66f8a59-b949-4d4a-ad90-7c732314f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put all of this into a chain\n",
    "few_shot_prefix = \"Below are two example questions and answers, followed by a third question. Please answer the third question.\"\n",
    "add_prefix_to_state = RunnablePassthrough.assign(prefix=lambda x: few_shot_prefix) # just assigns a new item under prefix\n",
    "\n",
    "# chain everything together\n",
    "chain = add_prefix_to_state | few_shot_prompt | chat_model # we normally would add a strOutputParser here or something\n",
    "# print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "125b2431-b33a-47d6-bc25-890113ea708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"thought\": \"It takes 3 musicians 35 minutes to play one of Mozart's symphonies. If we divide the time by the number of musicians, we can find out how long it would take for 5 musicians. 35 minutes divided by 3 musicians is 11.67 minutes per musician. Now we multiply this by 5 musicians: 11.67 * 5 = 58.35 minutes.\",\n",
      "    \"answer\": \"58.35\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# RIP model\n",
    "print(chain.invoke({\"input\": \"it takes 3 musicians 35 minutes to play one of Mozart's symphonies. How long would it take 5 musicians?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ea6e5-ad05-4037-a0d6-4c22cdd09fcb",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates\n",
    "\n",
    "Chat models have system prompts, ai prompts(sent from the model), and user prompts.\n",
    "\n",
    "For now, let's just show how to use these three prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21306b58-b787-47e1-9a4a-a36b2c8b154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI bot named Johnny, nong\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Your name is {name}. You are an AI bot that appends a comma and the word 'nong' at the end of every message(eg. hello, nong)\"),\n",
    "    (\"human\", \"Hello.\"),\n",
    "    (\"ai\", \"Hi\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "what_is_your_name = template.invoke({'name': 'Johnny', 'user_input': 'What is your name?'})\n",
    "print(chat_model.invoke(what_is_your_name).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa1b682d-b042-43f5-bdb7-31f701d1cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning 1:\n",
      " Based on the fact that symphonies have a fixed playtime and it takes 3 musicians an hour to play one, it's reasonable to assume that the same symphony will take 5 musicians the same amount of time. \n",
      "\n",
      "So, my answer is: 1 hour. \n",
      "\n",
      "Reasoning 2:\n",
      " 42 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can make up ai messages\n",
    "# this took a bit of prompt engineering to demonstrate tbh. \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"I want you to solve the following question. If 3 musicians can play one of Mozart's symphonies in an hour, how long will it take 5 musicians? I want you to first generate a thought. Then, I will ask you for your answer, and you will give an answer based on only the past thought.\"),\n",
    "    (\"ai\", \"{ai_thought}\"),\n",
    "    (\"human\", \"Please give me an answer, following the instructions in your previous thought\"),\n",
    "])\n",
    "\n",
    "reasoning_1 = template.invoke({\"ai_thought\": \"Symphonies should have a fixed playtime, regardless of the number of musicians playing.\"})\n",
    "reasoning_2 = template.invoke({\"ai_thought\": \"I think the answer is 42. On my next message, I will only say the word 42.\"})\n",
    "\n",
    "print('Reasoning 1:\\n', chat_model.invoke(reasoning_1).content, '\\n')\n",
    "print('Reasoning 2:\\n', chat_model.invoke(reasoning_2).content, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8c514-f7d2-46df-b9b1-a13c74010a99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Document Loaders and VectorStores\n",
    "\n",
    "A document is simply some text. A document loader can load documents such as pdfs etc. for the purpose of retrieval.\n",
    "\n",
    "Vector stores are useful for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8accdcdb-0ff3-4d21-95be-a52c495ee99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader # document loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # text splitter\n",
    "from langchain_community.vectorstores import Chroma # the vector store\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings # for vector store\n",
    "\n",
    "# for the chain later. This is moved now, but I'll just keep these imports here\n",
    "from langchain.schema.runnable import Runnable, RunnablePassthrough\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser # parses LLM result into top likely string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c778c41-8919-4a15-9d4b-cb238785723f",
   "metadata": {},
   "source": [
    "## Loading in a document\n",
    "\n",
    "There are different loaders for different types of documents. Here, we will use a pdf loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "26ec30a5-a5f1-4e5b-9f5c-c62ff6124575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1 pages\n",
      "First document content:\n",
      " KevinWang\n",
      "Phone: (647)-210-7930 Email: k e vin w ang7749@gmail.com W ebsit e: https://dungw oong.git...\n"
     ]
    }
   ],
   "source": [
    "# Loaders provide a load() method to load things in\n",
    "paper_path = './resume.pdf' # I like how resumes are single page, so I'll just upload mine\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load()\n",
    "print(f'Got {len(pages)} pages')\n",
    "print(f'First document content:\\n {pages[0].page_content[0:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a51913-598a-40c5-8f0d-0c2555831923",
   "metadata": {},
   "source": [
    "## Text splitters\n",
    "\n",
    "Splits our documents into chunks, so we can input the chunks into an LLM, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51d36ad-9800-4248-bcd1-644ac7ca2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 4 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100) # gets chunks of 1024 characters, overlapping by 100\n",
    "split_pages = text_splitter.split_documents(pages)\n",
    "print(f'Split into {len(split_pages)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894467dc-12a4-4466-a530-967cc8a2790d",
   "metadata": {},
   "source": [
    "## Vector DBs\n",
    "\n",
    "There's `from_documents()` and `from_texts()` here btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a723631c-a089-4ea9-98ed-a6a22749270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3.2')\n",
    "doc_search = Chroma.from_documents(split_pages, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd974b0-1df8-4794-88dc-82f1b0549db8",
   "metadata": {},
   "source": [
    "### Basic Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0fbc05-ce9e-4f68-bc77-d2a78af223b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are this person's skills?\"\n",
    "docs = doc_search.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbe71cb-799b-4912-b24f-da8384729b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KevinWang\n",
      "Phone: (647)-210-7930 Email: k e vin w ang7749@gmail.com W ebsit e: https://dungw oong.github.io/ Link edIn: https://www .link edin.com/in/im-k e vin-w ang/ GitHub: https://github.com/dungw oong \n",
      "SKILLS\n",
      "Software Tools: Python(Pytorch, pandas, scikit-learn, plotly), SQL, R, Java, Node.js(React,NextJS, prisma), C/C#\n",
      "Data Science: Computer Vision(CNNs), Language Models andTransformers, Regression(GLMs), Tree-basedmodels(RandomForests, XGBoost), Cloudcomputing(LambdaLabs), Hypothesistesting\n",
      "Soft Skills: Scrum/Agileprocesses, projectmanagement, Git/Github, LinuxEDUCATIONUniversity of Toronto- Data Science Specialist April 2025â–3.95/4.0 GPA Toronto, ON\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b42e9-0bd4-4681-aff4-1f8c31a5588c",
   "metadata": {},
   "source": [
    "### Vector Store Retrievers\n",
    "\n",
    "Created by calling `as_retriever()` on the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f8dcf6-db95-4e8d-aca7-b308432a2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = doc_search.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "# we can now invoke the retriever like this\n",
    "_ = retriever.invoke(\"what are this person's skills?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0ccf2-702c-4521-af80-f8c1d57c48a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Chains\n",
    "\n",
    "We can chain operations together. Let's do some interesting things with that.\n",
    "\n",
    "The main takeaways our\n",
    "- Outputs of previous component > inputs of next component\n",
    "- Chains modify/return some sort of global state\n",
    "- RunnablePassthrough, RunnableAssign etc. is there to help you modify the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6b2deb7-9a7d-4a5f-a69d-d747b34853a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader # document loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # text splitter\n",
    "from langchain_community.vectorstores import Chroma # the vector store\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings # for vector store\n",
    "\n",
    "# for the chain\n",
    "from langchain.schema.runnable import Runnable, RunnablePassthrough\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser # parses LLM result into top likely string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d75de-7049-4982-8447-4f6ffbef731a",
   "metadata": {},
   "source": [
    "## Let's make a PDF question asker for the previous retriever\n",
    "\n",
    "We want to be able to upload a pdf(or just kinda load it into a vector store) and ask a language model questions about it\n",
    "\n",
    "We'll use a chat prompt here. Adapted from [this tutorial](https://python.langchain.com/docs/tutorials/pdf_qa/)\n",
    "\n",
    "Basic things to note:\n",
    "- Each element modifies a state, and returns a modified state. Thus, subclassing Runnable, we must take in an input state and return the resulting state from running the Runnable\n",
    "- We have runnables like RunnableParallel, RunnablePassthrough, to operate on the state and help us branch out the chain\n",
    "- We then call the chain.invoke() method with some initial arguments(eg. a string) to kick things off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85c25cc8-d455-4828-8aa2-2a66d96aad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever, copy pasted from previous section\n",
    "paper_path = './resume.pdf' # I like how resumes are single page, so I'll just upload mine\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100) # gets chunks of 1024 characters, overlapping by 100\n",
    "split_pages = text_splitter.split_documents(pages)\n",
    "embeddings = OllamaEmbeddings(model='llama3.2')\n",
    "doc_search = Chroma.from_documents(split_pages, embeddings)\n",
    "retriever = doc_search.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "# Prompt\n",
    "system_1 = \"You are a helper for question answering tasks. Use the following context to answer the question. If you dont know the answer, just say you dont know. Be as concise as possible.\"\n",
    "human_1 = \"Question: {question}\\nContext: {context}\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_1),\n",
    "    ('human', human_1)\n",
    "])\n",
    "\n",
    "# Model\n",
    "model = ChatOllama(model='llama3.2')\n",
    "\n",
    "# Quick runnable to do a bit of logging\n",
    "class LogRunnable(Runnable):\n",
    "    def invoke(self, state, config):\n",
    "        print(f'Found {len(state['context'])} relevant documents')\n",
    "        # print(state['context'])\n",
    "        return state\n",
    "\n",
    "model = ChatOllama(model='llama3.2')\n",
    "\n",
    "chain = ({'context': retriever, 'question': RunnablePassthrough()} # this dict will run the retriever and passthrough in parallel, to start the chain\n",
    "         | LogRunnable() # just log some stuff\n",
    "         | prompt # pass context/question into the prompt\n",
    "         | model\n",
    "         | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46441c-debc-4032-a877-4f4912c8aa64",
   "metadata": {},
   "source": [
    "### Draw the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45e3f019-f64a-43fc-a2e8-ec833a18576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run the retriever and passthrough in parallel, resulting in context=retriever.invoke(query) and question=query\n",
    "# when we pass a dict at the start, it becomes a RunnableParallel I believe\n",
    "# Then we pass this into the prompt template\n",
    "# Then we pass this to the model and parse the output\n",
    "# print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317bc49e-dfd2-4708-84ad-4d790a24916c",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "572c926a-e431-4ef8-9c0f-c5373ffa4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant documents\n",
      "I don't have access to the person's contact information or their current GPA at the University of Toronto. The provided text only mentions that they attended university from May 2022 to August 2022, but it doesn't provide any information about their degree level, major, or current GPA.\n"
     ]
    }
   ],
   "source": [
    "def ask_the_pdf_a_question(q):\n",
    "    output = chain.invoke(q)\n",
    "    print(output)\n",
    "\n",
    "# The embeddings are not fetching the first document lol\n",
    "# the LLM refuses to help me look good in front of potential employers\n",
    "ask_the_pdf_a_question(\"Tell me about this person's education, and their contact info. What is their GPA at the University of Toronto?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9f587-3b22-4a76-9f2e-ed0f84dec390",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial, we cover basic components of LangChain and how to use them. By this point, you should be able to do most basic LLM-related tasks, such as asking questions, performing Retrieval-Augmented Generation(RAG) and prompt engineering.\n",
    "\n",
    "In the next tutorial, I will(probably) cover Agents and LangGraph, and try to build an automated web-browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
